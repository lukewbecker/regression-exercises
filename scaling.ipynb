{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of file.\n",
      "wrangle.py functions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import acquire\n",
    "import wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>monthly_charges</th>\n",
       "      <th>tenure</th>\n",
       "      <th>total_charges</th>\n",
       "      <th>contract_type_id</th>\n",
       "      <th>contract_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016-QLJIS</td>\n",
       "      <td>90.45</td>\n",
       "      <td>65</td>\n",
       "      <td>5957.9</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0017-DINOC</td>\n",
       "      <td>45.20</td>\n",
       "      <td>54</td>\n",
       "      <td>2460.55</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0019-GFNTW</td>\n",
       "      <td>45.05</td>\n",
       "      <td>56</td>\n",
       "      <td>2560.1</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0056-EPFBG</td>\n",
       "      <td>39.40</td>\n",
       "      <td>20</td>\n",
       "      <td>825.4</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0078-XZMHT</td>\n",
       "      <td>85.15</td>\n",
       "      <td>72</td>\n",
       "      <td>6316.2</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>9950-MTGYX</td>\n",
       "      <td>20.30</td>\n",
       "      <td>28</td>\n",
       "      <td>487.95</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>9953-ZMKSM</td>\n",
       "      <td>25.25</td>\n",
       "      <td>63</td>\n",
       "      <td>1559.3</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>9964-WBQDJ</td>\n",
       "      <td>24.40</td>\n",
       "      <td>71</td>\n",
       "      <td>1725.4</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>9972-EWRJS</td>\n",
       "      <td>19.25</td>\n",
       "      <td>67</td>\n",
       "      <td>1372.9</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>9975-GPKZU</td>\n",
       "      <td>19.75</td>\n",
       "      <td>46</td>\n",
       "      <td>856.5</td>\n",
       "      <td>3</td>\n",
       "      <td>Two year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1695 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id  monthly_charges  tenure total_charges  contract_type_id  \\\n",
       "0     0016-QLJIS            90.45      65        5957.9                 3   \n",
       "1     0017-DINOC            45.20      54       2460.55                 3   \n",
       "2     0019-GFNTW            45.05      56        2560.1                 3   \n",
       "3     0056-EPFBG            39.40      20         825.4                 3   \n",
       "4     0078-XZMHT            85.15      72        6316.2                 3   \n",
       "...          ...              ...     ...           ...               ...   \n",
       "1690  9950-MTGYX            20.30      28        487.95                 3   \n",
       "1691  9953-ZMKSM            25.25      63        1559.3                 3   \n",
       "1692  9964-WBQDJ            24.40      71        1725.4                 3   \n",
       "1693  9972-EWRJS            19.25      67        1372.9                 3   \n",
       "1694  9975-GPKZU            19.75      46         856.5                 3   \n",
       "\n",
       "     contract_type  \n",
       "0         Two year  \n",
       "1         Two year  \n",
       "2         Two year  \n",
       "3         Two year  \n",
       "4         Two year  \n",
       "...            ...  \n",
       "1690      Two year  \n",
       "1691      Two year  \n",
       "1692      Two year  \n",
       "1693      Two year  \n",
       "1694      Two year  \n",
       "\n",
       "[1695 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wrangle.get_telco_data_two_year()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1695 entries, 0 to 1694\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customer_id       1695 non-null   object \n",
      " 1   monthly_charges   1695 non-null   float64\n",
      " 2   tenure            1695 non-null   int64  \n",
      " 3   total_charges     1695 non-null   object \n",
      " 4   contract_type_id  1695 non-null   int64  \n",
      " 5   contract_type     1695 non-null   object \n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 92.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = wrangle.prep_acquired_telco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial visualization of the data:\n",
    "\n",
    "train.plot.scatter(y = 'tenure', x = 'monthly_charges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.total_charges.plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've already split the datasets into train, validate and test, so "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Apply the scalers we talked about in this lesson to your data and visualize the results in a way you find helpful.\n",
    "\n",
    "Make visualizations that enhance my own understanding of the data. Can be the ones used in the lesson, or can be my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the scaler object:\n",
    "\n",
    "scaler_minmax = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "# 2. Fitting ONLY do the train dataset. Remember, this is only useful on continuous variables, not categorical variables:\n",
    "scaler_minmax.fit(train[['total_charges', 'monthly_charges', 'tenure']])\n",
    "\n",
    "# 3. Now use the object on all three datasets. Remember, I'm using this object which was fitted to the train dataset:\n",
    "train[['total_charges', 'monthly_charges', 'tenure']] = scaler_minmax.transform(train[['total_charges', 'monthly_charges', 'tenure']])\n",
    "validate[['total_charges', 'monthly_charges', 'tenure']] = scaler_minmax.transform(validate[['total_charges', 'monthly_charges', 'tenure']])\n",
    "test[['total_charges', 'monthly_charges', 'tenure']] = scaler_minmax.transform(test[['total_charges', 'monthly_charges', 'tenure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how the x axis range is between 0 and 1.\n",
    "\n",
    "train.total_charges.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, the x axis has changed, but the shape of the data remains the same.\n",
    "\n",
    "train.plot.scatter(y = 'tenure', x = 'monthly_charges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data = train, x = \"total_charges\", y = \"tenure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.inverse_transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the standard scaling process to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimporting data under another train/validate/test dataset:\n",
    "\n",
    "train2, validate2, test2 = wrangle.prep_acquired_telco()\n",
    "train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.total_charges.plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the scaler object:\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# 2. Fitting ONLY do the train dataset. Remember, this is only useful on continuous variables, not categorical variables:\n",
    "scaler.fit(train2[['total_charges', 'monthly_charges', 'tenure']])\n",
    "\n",
    "# 3. Now use the object on all three datasets. Remember, I'm using this object which was fitted to the train dataset:\n",
    "train2[['total_charges', 'monthly_charges', 'tenure']] = scaler.transform(train2[['total_charges', 'monthly_charges', 'tenure']])\n",
    "validate2[['total_charges', 'monthly_charges', 'tenure']] = scaler.transform(validate2[['total_charges', 'monthly_charges', 'tenure']])\n",
    "test2[['total_charges', 'monthly_charges', 'tenure']] = scaler.transform(test2[['total_charges', 'monthly_charges', 'tenure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.total_charges.plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the RobustScaler:\n",
    "\n",
    "# Reimporting data under another train/validate/test dataset:\n",
    "\n",
    "train3, validate3, test3 = wrangle.prep_acquired_telco()\n",
    "train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3.total_charges.plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the scaler object:\n",
    "\n",
    "scaler_robust = sklearn.preprocessing.RobustScaler()\n",
    "\n",
    "# 2. Fitting ONLY do the train dataset. Remember, this is only useful on continuous variables, not categorical variables:\n",
    "scaler_robust.fit(train2[['total_charges', 'monthly_charges', 'tenure']])\n",
    "\n",
    "# 3. Now use the object on all three datasets. Remember, I'm using this object which was fitted to the train dataset:\n",
    "train3[['total_charges', 'monthly_charges', 'tenure']] = scaler_robust.transform(train3[['total_charges', 'monthly_charges', 'tenure']])\n",
    "validate3[['total_charges', 'monthly_charges', 'tenure']] = scaler_robust.transform(validate3[['total_charges', 'monthly_charges', 'tenure']])\n",
    "test3[['total_charges', 'monthly_charges', 'tenure']] = scaler_robust.transform(test3[['total_charges', 'monthly_charges', 'tenure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3.total_charges.plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the .inverse_transform method to your scaled data. Is the resulting dataset the exact same as the original data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the StandardScaler:\n",
    "\n",
    "# Reimporting data under another train/validate/test dataset:\n",
    "\n",
    "train4, validate4, test4 = wrangle.prep_acquired_telco()\n",
    "train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4.tenure.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_mm = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "scaler_mm.fit(train4[['tenure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4.tenure.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4['tenure_scaled'] = scaler_mm.transform(train4[['tenure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4.tenure_scaled.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4[['tenure_invert']] = scaler_mm.inverse_transform(train4[['tenure_scaled']])\n",
    "train4.tenure_invert.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Read the documentation for sklearn's QuantileTransformer. Use normal for the output_distribution and apply this scaler to your data. Visualize the result of your data scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.preprocessing.QuantileTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the QuantileTransformer:\n",
    "\n",
    "# Reimporting data under another train/validate/test dataset:\n",
    "\n",
    "train5, validate5, test5 = wrangle.prep_acquired_telco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the scaler object:\n",
    "\n",
    "scaler_qt = sklearn.preprocessing.QuantileTransformer(n_quantiles = 5, random_state = 123, output_distribution = 'normal')\n",
    "\n",
    "# 2. Fitting ONLY do the train dataset. Remember, this is only useful on continuous variables, not categorical variables:\n",
    "scaler_qt.fit(train5[['total_charges', 'monthly_charges', 'tenure']])\n",
    "\n",
    "# 3. Now use the object on all three datasets. Remember, I'm using this object which was fitted to the train dataset:\n",
    "train5[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_qt.transform(train5[['total_charges', 'monthly_charges', 'tenure']])\n",
    "validate5[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_qt.transform(validate5[['total_charges', 'monthly_charges', 'tenure']])\n",
    "test5[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_qt.transform(test5[['total_charges', 'monthly_charges', 'tenure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5.tenure_scaled.plot.hist()\n",
    "\n",
    "# It produced an approximately normal distribution, except for the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use the QuantileTransformer, but omit the output_distribution argument. Visualize your results. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the QuantileTransformer:\n",
    "\n",
    "# Reimporting data under another train/validate/test dataset:\n",
    "\n",
    "train5, validate5, test5 = wrangle.prep_acquired_telco()\n",
    "train5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the scaler object:\n",
    "\n",
    "scaler_qt = sklearn.preprocessing.QuantileTransformer(n_quantiles = 5, random_state = 123)\n",
    "\n",
    "# n_quantiles = 5, random_state = 123, output_distribution = 'normal'\n",
    "\n",
    "# 2. Fitting ONLY do the train dataset. Remember, this is only useful on continuous variables, not categorical variables:\n",
    "scaler_qt.fit(train5[['total_charges', 'monthly_charges', 'tenure']])\n",
    "\n",
    "# 3. Now use the object on all three datasets. Remember, I'm using this object which was fitted to the train dataset:\n",
    "train5[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_qt.transform(train5[['total_charges', 'monthly_charges', 'tenure']])\n",
    "validate5[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_qt.transform(validate5[['total_charges', 'monthly_charges', 'tenure']])\n",
    "test5[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_qt.transform(test5[['total_charges', 'monthly_charges', 'tenure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5.tenure_scaled.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default output of the function is to use a uniform distribution, and that's what this output appears to be.\n",
    "\n",
    "The difference is in the shape of the distribution; this distribution has a uniform distribution as the scaler was told to produce. The previous distribution, while not perfection normal, had an identifibly normal shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Based on the work you've done, choose a scaling method for your dataset. \n",
    "- Write a function within your wrangle.py that accepts as input the train, validate, and test data splits, and returns the scaled versions of each. \n",
    "- Be sure to only learn the parameters for scaling from your training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the StandardScaler:\n",
    "\n",
    "# Reimporting data under another train/validate/test dataset:\n",
    "\n",
    "train, validate, test = wrangle.prep_acquired_telco()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the scaler object:\n",
    "\n",
    "scaler_ss = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# 2. Fitting ONLY do the train dataset. Remember, this is only useful on continuous variables, not categorical variables:\n",
    "scaler_ss.fit(train[['total_charges', 'monthly_charges', 'tenure']])\n",
    "\n",
    "# 3. Now use the object on all three datasets. Remember, I'm using this object which was fitted to the train dataset:\n",
    "train[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_ss.transform(train[['total_charges', 'monthly_charges', 'tenure']])\n",
    "validate[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_ss.transform(validate[['total_charges', 'monthly_charges', 'tenure']])\n",
    "test[['total_charges_scaled', 'monthly_charges_scaled', 'tenure_scaled']] = scaler_ss.transform(test[['total_charges', 'monthly_charges', 'tenure']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_telco_data_all(path):\n",
    "    # First, I need to acquire the dataframe within this function:\n",
    "    df = pd.read_csv('telco_customers_df_two_year.csv', index_col = 0)\n",
    "\n",
    "    # Cleaning the total_costs column by dropping empty values:\n",
    "    df.drop(df[df['total_charges'] == \" \"].index, inplace = True)\n",
    "\n",
    "    # Changing the total_costs column to a float:\n",
    "    df['total_charges'] = df.total_charges.astype('float')\n",
    "\n",
    "    # get object column names\n",
    "    object_cols = get_object_cols(df)\n",
    "    \n",
    "    # create dummy vars\n",
    "    df = create_dummies(df, object_cols)\n",
    "      \n",
    "    # split data \n",
    "    X_train, y_train, X_validate, y_validate, X_test, y_test = train_validate_test(df, 'G3')\n",
    "    \n",
    "    # get numeric column names\n",
    "    numeric_cols = get_numeric_X_cols(X_train, object_cols)\n",
    "\n",
    "    # scale data \n",
    "    X_train_scaled, X_validate_scaled, X_test_scaled = min_max_scale(X_train, X_validate, X_test, numeric_cols)\n",
    "    \n",
    "    return df, X_train, X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test(df, target):\n",
    "    '''\n",
    "    this function takes in a dataframe and splits it into 3 samples, \n",
    "    a test, which is 20% of the entire dataframe, \n",
    "    a validate, which is 24% of the entire dataframe,\n",
    "    and a train, which is 56% of the entire dataframe. \n",
    "    It then splits each of the 3 samples into a dataframe with independent variables\n",
    "    and a series with the dependent, or target variable. \n",
    "    The function returns 3 dataframes and 3 series:\n",
    "    X_train (df) & y_train (series), X_validate & y_validate, X_test & y_test. \n",
    "    '''\n",
    "    # split df into test (20%) and train_validate (80%)\n",
    "    train_validate, test = train_test_split(df, test_size=.2, random_state=123)\n",
    "\n",
    "    # split train_validate off into train (70% of 80% = 56%) and validate (30% of 80% = 24%)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, random_state=123)\n",
    "\n",
    "        \n",
    "    # split train into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_train = train.drop(columns=[target])\n",
    "    y_train = train[target]\n",
    "    \n",
    "    # split validate into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_validate = validate.drop(columns=[target])\n",
    "    y_validate = validate[target]\n",
    "    \n",
    "    # split test into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_test = test.drop(columns=[target])\n",
    "    y_test = test[target]\n",
    "    \n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X_train, \\\n",
    "    X_train_scaled, y_train, \\\n",
    "    X_validate_scaled, y_validate, \\\n",
    "    X_test_scaled, y_test = train_validate_test(df, 'tenure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
